import numpy as np
import random
from trail import load_santa_fe_trail
import os

# --- Constants and Hyperparameters ---
# Environment
GRID_SIZE = 32
MAX_STEPS_PER_EPISODE = 50
TOTAL_FOOD_PELLETS_ON_MAP = 89 
START_POS = (0, 0)  # top-left
START_ORIENTATION = 'EAST' # Example, adjust based on map (0:E, 1:S, 2:W, 3:N)

# SNN Parameters
FLIF_MEMBRANE_TIME_CONSTANT = 20.0  # ms
FLIF_THRESHOLD_VOLTAGE = 0.7    # mV (or normalized units)
FLIF_RESET_VOLTAGE = 0.0    # mV
FLIF_NEURONS_BIAS = 0.05    # Small bias
FLIF_FRACTIONAL_ORDER_ALPHA = float(os.environ.get("ALPHA", 0.4))
FLIF_MEMORY_LENGTH = 12500 # full context for 250 total steps (change if necessary for longer simulations)

LIF_MEMBRANE_TIME_CONSTANT = 20.0  # ms
LIF_THRESHOLD_VOLTAGE = 0.75        # mV
LIF_RESET_VOLTAGE = 0.0          # mV
LIF_NEURONS_BIAS = 0.05

DT_NEURON_SIM = 0.1              # ms
T_ANT_DECISION_WINDOW = 5.0       # ms
NUM_NEURON_STEPS_PER_ANT_STEP = int(T_ANT_DECISION_WINDOW / DT_NEURON_SIM)

I_ACTIVE_INPUT_CURRENT = 1.5     # Current injected when context is active

NUM_HIDDEN_NEURONS = 16 # Number of hidden neurons (duh)

# RL Parameters
LEARNING_RATE_ETA = 0.0001
DISCOUNT_FACTOR_GAMMA = 0.99
EXPLORATION_TEMPERATURE_TAU_RL = 0.5 

# Surrogate Gradient
SG_RECT_WIDTH = 1.2             # mV

# Action Mapping
ACTION_MAP = {0: 'TurnLeft', 1: 'TurnRight', 2: 'MoveForward'}
ACTION_IDX_MAP = {'TurnLeft':0, 'TurnRight':1, 'MoveForward':2} # For convenience

NUM_EPISODES = 1000 # Example number of training episodes

# --- Helper Functions ---
def calculate_gl_coefficients(alpha, length):
    coeffs = np.zeros(length, dtype=np.float64)
    if length == 0:
        return coeffs
    coeffs[0] = -alpha 
    for j in range(1, length):
        coeffs[j] = (1.0 - (alpha + 1.0) / (j + 1.0)) * coeffs[j-1]  
    return coeffs

def softmax_stable(logits_array):
    if not logits_array.size: return np.array([]) # Handle empty array
    stable_logits = logits_array - np.max(logits_array)
    exp_logits = np.exp(stable_logits)
    sum_exp_logits = np.sum(exp_logits)
    if sum_exp_logits == 0: # Avoid division by zero if all logits are extremely small
        return np.ones_like(exp_logits) / exp_logits.size
    return exp_logits / sum_exp_logits

def calculate_discounted_returns(rewards_list, gamma):
    G = 0.0
    discounted_returns = np.zeros_like(rewards_list, dtype=float)
    for t in reversed(range(len(rewards_list))):
        G = rewards_list[t] + gamma * G
        discounted_returns[t] = G
    return discounted_returns

def rectangular_surrogate_gradient(membrane_potential, threshold, width):
    u = membrane_potential - threshold
    if abs(u) < width / 2.0:
        return 1.0 / width
    return 0.0

def initialize_weights(num_weights):
    return (np.random.rand(*num_weights)) #* 0.5 + 0.1

# --- Neuron Classes ---
class FractionalLIFNeuron:
    def __init__(self, neuron_id, params):
        self.neuron_id = neuron_id
        self.alpha = params.get("alpha", FLIF_FRACTIONAL_ORDER_ALPHA)
        self.tau_m = params.get("tau_m", FLIF_MEMBRANE_TIME_CONSTANT)
        self.V_th = params.get("V_th", FLIF_THRESHOLD_VOLTAGE)
        self.V_reset = params.get("V_reset", FLIF_RESET_VOLTAGE)
        self.bias = params.get("bias", FLIF_NEURONS_BIAS)
        self.memory_length = params.get("memory_length", FLIF_MEMORY_LENGTH)
        
        self.V = self.V_reset
        self.voltage_history = np.full(self.memory_length, self.V_reset, dtype=np.float64)
        self.gl_coefficients = calculate_gl_coefficients(self.alpha, self.memory_length)
        self.spike_state = 0

    def reset_state(self):
        self.V = self.V_reset
        self.voltage_history.fill(self.V_reset)
        self.spike_state = 0

    def update(self, input_current, dt):
        self.spike_state = 0
        
        history_component = 0.0
        if self.memory_length > 0:
            history_component = np.dot(self.gl_coefficients, self.voltage_history)

        kernel = dt**self.alpha
        
        effective_dV_dt_part = (-self.V / self.tau_m) + self.bias + input_current
        
        self.V = effective_dV_dt_part * kernel - history_component

        if self.V >= self.V_th:
            self.spike_state = 1
            self.V = self.V_reset
        
        # Update voltage_history (roll and add new V)
        if self.memory_length > 0:
            self.voltage_history = np.roll(self.voltage_history, 1)
            self.voltage_history[0] = self.V # Store post-reset or current subthreshold V

    def get_spike_state(self):
        return self.spike_state

    def get_voltage(self):
        return self.V


### Non-fractional neuron model ###
class StandardLIFNeuron:
    def __init__(self, neuron_id, params):
        self.neuron_id = neuron_id
        self.tau_m = params.get("tau_m", LIF_MEMBRANE_TIME_CONSTANT)
        self.V_th = params.get("V_th", LIF_THRESHOLD_VOLTAGE)
        self.V_reset = params.get("V_reset", LIF_RESET_VOLTAGE)
        self.bias_current = params.get("bias_current", LIF_NEURONS_BIAS) # Assuming bias is a current
        
        self.V = self.V_reset
        self.spike_state = 0

    def reset_state(self):
        self.V = self.V_reset
        self.spike_state = 0

    def update(self, input_current, dt):
        self.spike_state = 0
        # dV/dt = (-V + V_rest + bias_current*R_m + input_current*R_m) / tau_m
        # Assuming V_rest = 0 and R_m is absorbed into currents or tau_m definition.
        # More simply: dV/dt = (-V/tau_m) + (bias_current + input_current)/C_m
        # If tau_m = R_m * C_m, then dV/dt = (-V + R_m*(bias_current + input_current))/tau_m
        # Let's assume bias is a current and input_current is also a current.
        # dV = ((-self.V / self.tau_m) + self.bias_current + input_current) * dt 
        # A common discrete form:
        alpha_decay = np.exp(-dt / self.tau_m)
        self.V = self.V * alpha_decay + (self.bias_current + input_current) * (1 - alpha_decay) * self.tau_m # if tau_m is R*C and I is current
        # Simpler Euler:
        # dV_dt = (-self.V + self.bias_current*self.tau_m + input_current*self.tau_m) / self.tau_m # if bias is a voltage-like term
        dV_dt = (-self.V / self.tau_m) + self.bias_current + input_current # if bias and input_current are currents scaled by 1/C
        self.V += dV_dt * dt


        if self.V >= self.V_th:
            self.spike_state = 1
            self.V = self.V_reset
            
    def get_spike_state(self):
        return self.spike_state

    def get_voltage(self):
        return self.V

# --- Environment Class ---
class SantaFeEnvironment:
    def __init__(self, map_filepath, start_pos, start_orientation_str):
        self.trail_map_original = self._load_map(map_filepath)
        self.trail_map_current = np.copy(self.trail_map_original)
        self.start_pos = start_pos
        self.start_orientation_str = start_orientation_str # 'EAST', 'SOUTH', 'WEST', 'NORTH'
        self.orientations = ['EAST', 'SOUTH', 'WEST', 'NORTH']
        self.orientation_deltas = { # dx, dy
            'EAST': (1, 0), 'SOUTH': (0, 1), 'WEST': (-1, 0), 'NORTH': (0, -1)
        }
        self.ant_pos = None
        self.ant_orientation_idx = None # Index in self.orientations
        self.food_eaten_in_episode = 0
        self.grid_height, self.grid_width = self.trail_map_original.shape

    def _load_map(self, filepath):
        simple_map = np.zeros((5,5), dtype=int)
        simple_map[0,1] = 1; simple_map[0,2] = 1; simple_map[0,3] = 1;
        return load_santa_fe_trail()

    def reset_ant_and_trail(self):
        self.trail_map_current = np.copy(self.trail_map_original)
        self.ant_pos = list(self.start_pos)
        self.ant_orientation_idx = self.orientations.index(self.start_orientation_str)
        self.food_eaten_in_episode = 0
        return tuple(self.ant_pos), self.orientations[self.ant_orientation_idx], np.sum(self.trail_map_current)

    def get_food_ahead(self):
        dx, dy = self.orientation_deltas[self.orientations[self.ant_orientation_idx]]
        front_x, front_y = self.ant_pos[0] + dx, self.ant_pos[1] + dy
        
        if 0 <= front_x < self.grid_width and 0 <= front_y < self.grid_height:
            return self.trail_map_current[front_y, front_x] == 1 # Assuming map is (y,x)
        return False # Off grid means no food

    def step(self, action_idx): # action_idx: 0:L, 1:R, 2:Fwd
        action_str = ACTION_MAP[action_idx]
        reward = -0.005 # Default step cost
        episode_done_env = False
        food_consumed_flag = False
        prev_pos = list(self.ant_pos)

        if action_str == 'TurnLeft':
            self.ant_orientation_idx = (self.ant_orientation_idx - 1 + 4) % 4
        elif action_str == 'TurnRight':
            self.ant_orientation_idx = (self.ant_orientation_idx + 1) % 4
        elif action_str == 'MoveForward':
            dx, dy = self.orientation_deltas[self.orientations[self.ant_orientation_idx]]
            next_x, next_y = self.ant_pos[0] + dx, self.ant_pos[1] + dy

            if 0 <= next_x < self.grid_width and 0 <= next_y < self.grid_height:
                self.ant_pos = [next_x, next_y]
                if self.trail_map_current[next_y, next_x] == 1:
                    reward = 5.0 # Food reward
                    self.trail_map_current[next_y, next_x] = 0 # Eat food
                    self.food_eaten_in_episode += 1
                    food_consumed_flag = True
                    if self.food_eaten_in_episode == TOTAL_FOOD_PELLETS_ON_MAP: # Use actual total from loaded map
                        episode_done_env = True
            else:
                reward = -5.0 # heavy penalty for bumping walls
                pass
        
        return tuple(self.ant_pos), self.orientations[self.ant_orientation_idx], reward, episode_done_env, food_consumed_flag

# --- MAIN SIMULATION AND TRAINING LOOP ---

# Initialize SNN
flif_neuron_params = {
    "alpha": FLIF_FRACTIONAL_ORDER_ALPHA, "tau_m": FLIF_MEMBRANE_TIME_CONSTANT,
    "V_th": FLIF_THRESHOLD_VOLTAGE, "V_reset": FLIF_RESET_VOLTAGE,
    "bias": FLIF_NEURONS_BIAS, "memory_length": FLIF_MEMORY_LENGTH
}

# Context Neurons
fLIF_food = FractionalLIFNeuron("food_ctx", flif_neuron_params)
fLIF_nofood = FractionalLIFNeuron("nofood_ctx", flif_neuron_params)

# Hidden Layer
hidden_neurons = [FractionalLIFNeuron(f"hidden_{i}", flif_neuron_params) for i in range(NUM_HIDDEN_NEURONS)]

# Action Layer (Output)
action_leaf_neurons = [FractionalLIFNeuron(f"action_leaf_{i}", flif_neuron_params) for i in range(3)]

# Weights from Context (2) -> Hidden (N)
W_context_to_hidden = initialize_weights((2, NUM_HIDDEN_NEURONS))

# Weights from Hidden (N) -> Action (3)
W_hidden_to_action = initialize_weights((NUM_HIDDEN_NEURONS, 3)) 

# Initialize Environment
environment = SantaFeEnvironment("koza_trail.txt", START_POS, START_ORIENTATION) 

print(f"Starting training for {NUM_EPISODES} episodes...")
avg_food_eaten = 0
food_trace = []


for episode_i in range(NUM_EPISODES):
    ant_pos, ant_orient_str, _ = environment.reset_ant_and_trail()
    
    fLIF_food.reset_state()
    fLIF_nofood.reset_state()
    for hn in hidden_neurons:
        hn.reset_state()
    for leaf_neuron in action_leaf_neurons:
        leaf_neuron.reset_state()

    episode_trajectory = []
    total_episode_reward = 0.0
    food_eaten_this_episode = 0

    for t_ant_step in range(MAX_STEPS_PER_EPISODE):
        is_food_ahead = environment.get_food_ahead()

        active_fLIF = fLIF_food if is_food_ahead else fLIF_nofood
        inactive_fLIF = fLIF_nofood if is_food_ahead else fLIF_food
        
        current_input_to_active_fLIF = I_ACTIVE_INPUT_CURRENT
        current_input_to_inactive_fLIF = 0.0

        fLIF_spike_trace_this_T_ant = np.zeros(NUM_NEURON_STEPS_PER_ANT_STEP, dtype=int)
        hidden_spike_trace_this_T_ant = np.zeros((NUM_NEURON_STEPS_PER_ANT_STEP, NUM_HIDDEN_NEURONS), dtype=int) 
        hidden_potentials_this_T_ant = [np.zeros(NUM_NEURON_STEPS_PER_ANT_STEP) for _ in range(NUM_HIDDEN_NEURONS)] 
        leaf_potentials_this_T_ant = [np.zeros(NUM_NEURON_STEPS_PER_ANT_STEP) for _ in range(3)]
        current_T_ant_leaf_spike_counts = np.zeros(3, dtype=int)


            # --- Simulation Loop ---
        for t_neuron_idx in range(NUM_NEURON_STEPS_PER_ANT_STEP):
            # 1. Update Context Neurons
            active_fLIF.update(current_input_to_active_fLIF, DT_NEURON_SIM)
            inactive_fLIF.update(current_input_to_inactive_fLIF, DT_NEURON_SIM)
            fLIF_spike_trace_this_T_ant[t_neuron_idx] = active_fLIF.get_spike_state()
            
            # Store context spikes
            context_spikes_now = np.zeros(2)
            if is_food_ahead:
                context_spikes_now[0] = active_fLIF.get_spike_state()
            else:
                context_spikes_now[1] = active_fLIF.get_spike_state()

            # 2. Update Hidden Neurons
            # Calculate total current to hidden layer
            synaptic_current_to_hidden = np.dot(context_spikes_now, W_context_to_hidden)
            
            for i_hidden in range(NUM_HIDDEN_NEURONS):
                hidden_neurons[i_hidden].update(synaptic_current_to_hidden[i_hidden], DT_NEURON_SIM)
                spike_state = hidden_neurons[i_hidden].get_spike_state()
                hidden_spike_trace_this_T_ant[t_neuron_idx, i_hidden] = spike_state
                hidden_potentials_this_T_ant[i_hidden][t_neuron_idx] = hidden_neurons[i_hidden].get_voltage()

            # 3. Update Action Neurons
            # Calculate total current to action layer from hidden layer
            hidden_spikes_now = hidden_spike_trace_this_T_ant[t_neuron_idx, :]
            synaptic_current_to_action = np.dot(hidden_spikes_now, W_hidden_to_action)

            for i_leaf in range(3):
                action_leaf_neurons[i_leaf].update(synaptic_current_to_action[i_leaf], DT_NEURON_SIM)
                if action_leaf_neurons[i_leaf].get_spike_state() == 1:
                    current_T_ant_leaf_spike_counts[i_leaf] += 1
                leaf_potentials_this_T_ant[i_leaf][t_neuron_idx] = action_leaf_neurons[i_leaf].get_voltage()

        action_probabilities = softmax_stable(current_T_ant_leaf_spike_counts / EXPLORATION_TEMPERATURE_TAU_RL)
        
        # Handle case where all spike counts are zero -> uniform probabilities
        if np.sum(current_T_ant_leaf_spike_counts) == 0 :
             action_probabilities = np.ones(3) / 3.0

        chosen_action_idx = np.random.choice(3, p=action_probabilities)
        
        next_ant_pos, next_ant_orient_str, reward, episode_done_env, food_consumed_flag = \
            environment.step(chosen_action_idx)
        
        total_episode_reward += reward
        if food_consumed_flag:
             food_eaten_this_episode +=1
        
        episode_trajectory.append({
            "is_food_ahead_context": is_food_ahead,
            "context_spike_trace": np.copy(fLIF_spike_trace_this_T_ant), # Keeping this for now for simplicity
            "hidden_spike_trace": np.copy(hidden_spike_trace_this_T_ant),
            "hidden_potentials_traces": [np.copy(p_trace) for p_trace in hidden_potentials_this_T_ant],
            "leaf_potentials_traces": [np.copy(p_trace) for p_trace in leaf_potentials_this_T_ant],
            "chosen_action_idx": chosen_action_idx,
            "action_probabilities": np.copy(action_probabilities),
            "reward": reward
        })

        ant_pos, ant_orient_str = next_ant_pos, next_ant_orient_str # Update ant's state string for orientation
        if episode_done_env or food_eaten_this_episode == TOTAL_FOOD_PELLETS_ON_MAP:
            break

    rewards_for_G_t = [t["reward"] for t in episode_trajectory]
    discounted_returns_G_t = calculate_discounted_returns(rewards_for_G_t, DISCOUNT_FACTOR_GAMMA)
    if len(discounted_returns_G_t) > 1:
        mean_G_t = np.mean(discounted_returns_G_t)
        std_G_t = np.std(discounted_returns_G_t)
        if std_G_t > 1e-8: # Add a small epsilon to prevent division by zero if all G_t are the same
            normalized_G_t_values = (discounted_returns_G_t - mean_G_t) / std_G_t
        else:
            normalized_G_t_values = discounted_returns_G_t - mean_G_t # Just center if std is tiny
    elif len(discounted_returns_G_t) == 1:
        normalized_G_t_values = discounted_returns_G_t # Or set to 0 if only one step, as G_0 - mean(G_0) = 0
    else: # No transitions
        normalized_G_t_values = np.array([])

    # 1. Initialize gradient matrices for this episode
    delta_W_context_to_hidden = np.zeros_like(W_context_to_hidden)
    delta_W_hidden_to_action = np.zeros_like(W_hidden_to_action)

    # 2. Loop through each step of the episode in reverse (or forward, it's equivalent for this policy gradient form)
    for t_idx, transition in enumerate(episode_trajectory):
        G_t_to_use = normalized_G_t_values[t_idx]
        
        # --- Retrieve data for this transition ---
        chosen_action_idx = transition["chosen_action_idx"]
        action_probabilities = transition["action_probabilities"]
        
        # Reconstruct presynaptic spike activity for both layers
        context_spikes_t = np.zeros(2)
        if transition["is_food_ahead_context"]:
            context_spikes_t[0] = np.sum(transition["context_spike_trace"])
        else:
            context_spikes_t[1] = np.sum(transition["context_spike_trace"])
            
        hidden_spikes_t = np.sum(transition["hidden_spike_trace"], axis=0) # Sum spikes for each hidden neuron over the window

        # --- A. Calculate "error" at the OUTPUT layer ---
        # This is the classic REINFORCE signal: (did_I_take_action - probability_of_action)
        delta_output_layer = np.zeros(3)
        delta_output_layer[chosen_action_idx] = 1.0
        delta_output_layer -= action_probabilities
        
        # Modulate error by the surrogate gradient of the action neurons
        # This approximates how much a change in input would have affected the output spike count
        for k in range(3):
            sg_action = 0.0
            potentials = transition["leaf_potentials_traces"][k]
            for v in potentials:
                sg_action += rectangular_surrogate_gradient(v, flif_neuron_params["V_th"], SG_RECT_WIDTH)
            delta_output_layer[k] *= sg_action

        # --- B. Calculate gradient for W_hidden_to_action ---
        # Formula: ΔW_h_a += pre-synaptic_activity (hidden)  x  post-synaptic_error (output)
        # We use np.outer to multiply the hidden spike vector by the output error vector
        grad_W_hidden_to_action = np.outer(hidden_spikes_t, delta_output_layer)
        delta_W_hidden_to_action += grad_W_hidden_to_action * G_t_to_use

        # --- C. Backpropagate the error to the HIDDEN layer ---
        # The error at the hidden layer is the output error weighted by the connections
        error_at_hidden_layer = np.dot(delta_output_layer, W_hidden_to_action.T)
        
        # Modulate this backpropagated error by the surrogate gradient of the hidden neurons
        for j in range(NUM_HIDDEN_NEURONS):
            sg_hidden = 0.0
            potentials = transition["hidden_potentials_traces"][j]
            for v in potentials:
                sg_hidden += rectangular_surrogate_gradient(v, flif_neuron_params["V_th"], SG_RECT_WIDTH)
            error_at_hidden_layer[j] *= sg_hidden

        # --- D. Calculate gradient for W_context_to_hidden ---
        # Formula: ΔW_c_h += pre-synaptic_activity (context) x post-synaptic_error (hidden)
        grad_W_context_to_hidden = np.outer(context_spikes_t, error_at_hidden_layer)
        delta_W_context_to_hidden += grad_W_context_to_hidden * G_t_to_use

    # 3. Apply the accumulated gradients after the episode
    max_grad_abs_val = 1000.0  # I have tried many values here (1.0, 5.0, 10.0, 100.0, etc.)

    np.clip(delta_W_context_to_hidden, -max_grad_abs_val, max_grad_abs_val, out=delta_W_context_to_hidden)
    np.clip(delta_W_hidden_to_action, -max_grad_abs_val, max_grad_abs_val, out=delta_W_hidden_to_action)

    W_context_to_hidden += LEARNING_RATE_ETA * delta_W_context_to_hidden
    W_hidden_to_action += LEARNING_RATE_ETA * delta_W_hidden_to_action

    # Print updates every ten episodes
    avg_food_eaten += food_eaten_this_episode
    if (episode_i+1) % 10 == 0: 
        avg_food_eaten /= 10
        w_c_h_norm = np.linalg.norm(W_context_to_hidden)
        w_h_a_norm = np.linalg.norm(W_hidden_to_action)
        print(f"Episode {episode_i+1}: Steps={t_ant_step+1}, Average Food Eaten={avg_food_eaten}, Total Reward={total_episode_reward:.2f}, "
              f"|W_c_h|={w_c_h_norm:.3f}, |W_h_a|={w_h_a_norm:.3f}")
        avg_food_eaten = 0
    food_trace.append(food_eaten_this_episode)

print("Training finished. Saving data.")

filename = f"food_eaten_{FLIF_FRACTIONAL_ORDER_ALPHA}.dat"
np.savetxt(filename, food_trace)

print("Data saved.")

